# Description
[MACHINE LEARNING AND PATTERN RECOGNITION](http://www.inf.ed.ac.uk/teaching/courses/mlpr/2016/) - In other words, machine learning from bayesian perspective.  So the primary focus was on generative models. Coursework consisted of two parts and was done in Matlab. The report with relevant code pieces is located in [MLPRFeedback.pdf](https://github.com/rb-kuddai/MLPR/blob/master/report.pdf). The final mark was 98/100, the highest mark in the group of more than 50 people.

## First Task
It was regression task where we were asked to predict a target pixel value in the bottom of some tissue images. We compared two models: Neural Network (simple MLP) and line regression. The results for NN and linear regression were quite close, that is why I chose linear regression as my primal model, as it was a more simple model (Occam's razor principle). The only useful insight about dataset was that intensities of pixels near target one were correlated with it. Thus, I determined an effective radius of pixels that correlated mostly with target pixel via cross-validation. Once it was done I got one of lowest root-mean-square errors (RMSE) in the group.

## Second Task
It was about binary text classification. In 2.1c I noticed that simple linear classificator should have perfectly separate a dataset with less than 100 examples considering that dimensionality of data was more than 100 dimensions unless there was a problem with the dataset. Indeed there were cases of mislabels when 2 or more datapoints had absolutely identical features but different labels. In 2.2 it was interesting to see how accounting for data corruption via proper custom prior helped to improve overall classification results (pretty much identical to L1 or L2 but for sigmoid specifically).
